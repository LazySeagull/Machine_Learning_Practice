{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072d59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how is pytorch tensor different to numpy array and how is pytorch different from numpy since they are similar in many ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba083675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFirstly tensor and matrix are very different. Matrix is basically a subset of tensor cuz matrix is 2d while tenors in maths can be \\n1d ( scaler) , 2d , 3d , ... , nd. \\n\\nnow for pytorch tensor , it acts like a numpy array but is very different.\\nfirstly we can store it on gpu which we cant do with numpy array \\nnext they have .grad() property which we can use to calculate their graidents \\nthey store compuation graph for the operations performed on the tenosr if we want to stop it \\nfrom storing computation graph say when maybe we want to validate our model and we dont want the validation calculatioin \\nto hamper the model training computation graph , we can use with torch.no_grad() property. Moreover using \\n.backward() method allows us to perform autograd or backpropagation on the tensor by using the computation graph.\\n\\n\\npytorch is different from numpy as pytorch lets us build tensors which are similar yet different from numpy arays moreover pytorch lets \\nus perform backpropagation and provides both low and high level apis or methods for builidng and training our models.\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Firstly tensor and matrix are very different. Matrix is basically a subset of tensor cuz matrix is 2d while tenors in maths can be \n",
    "1d ( scaler) , 2d , 3d , ... , nd. \n",
    "\n",
    "now for pytorch tensor , it acts like a numpy array but is very different.\n",
    "firstly we can store it on gpu which we cant do with numpy array \n",
    "next they have .grad() property which we can use to calculate their graidents \n",
    "they store compuation graph for the operations performed on the tenosr if we want to stop it \n",
    "from storing computation graph say when maybe we want to validate our model and we dont want the validation calculatioin \n",
    "to hamper the model training computation graph , we can use with torch.no_grad() property. Moreover using \n",
    ".backward() method allows us to perform autograd or backpropagation on the tensor by using the computation graph.\n",
    "\n",
    "\n",
    "pytorch is different from numpy as pytorch lets us build tensors which are similar yet different from numpy arays moreover pytorch lets \n",
    "us perform backpropagation and provides both low and high level apis or methods for builidng and training our models.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a1383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff between torch.<method> and torch.<method>_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f05a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# basically stuff ending with _ is inplace that means they will change the original tensor while the stuff not ending with the _\n",
    "# means it will return a new tensor and wont modify the orignal one \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e546efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ed54a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ace90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.tensor([-1 , -2 , 3 ,4 ,5 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba9ffa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b98a5eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 3, 4, 5]) tensor([-1, -2,  3,  4,  5])\n"
     ]
    }
   ],
   "source": [
    "yeah = torch.relu(random_tensor)\n",
    "print(yeah , random_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ed007b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 4, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antoher = torch.relu_(random_tensor)\n",
    "antoher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a28041ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3, 4, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0386975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3 two ways to create tensor on gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51631187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first way is uh create tensor then do .to(device) and set device to cuda\n",
    "\n",
    "# next ig create a tensor then pass device argument and set it to cuda there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5928f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q4 three ways to perform tensor computation without using autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "318ae0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad()\n",
    "\n",
    "# in tensor write require_grad=False\n",
    "\n",
    "#.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79ab8742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #question 5\n",
    "\n",
    "# i think it will give a runtime error cuz see like we arent changing the original tensor when we apply .cos \n",
    "# so the autograd will be cooked ig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "542098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(2.0 , requires_grad=True   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db14574",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = t.cos().exp_()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d79dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nvm it worked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d648bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec28c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d444d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc870164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe333fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main steps in pytorch training loop : \n",
    "\n",
    "# model.train() to begin training initialization \n",
    "# do metric.reset() to flush any prev stuff stored\n",
    "# then iterate over each epoch in total epoch\n",
    "# then iterate over train x batch and train y batch from dataloader \n",
    "# do forward pass which is prediciting the y_pred or y_logits using the model which is initially using random parameters set using torch.manual_seed() \n",
    "# then we can find the loss using appropriate criterion provided in the function argument\n",
    "# then we can do our optimizer.zero_grad to set the gradients of all optimized var to 0 \n",
    "# then we can run loss.backward to run backpropagation \n",
    "# then we can run optimizer.step to tweak our weight parameters \n",
    "# then we can do metric.update to update our training metric so we can get idea about our model we can also print it \n",
    "# then we can append total loss which we need to initialize as 0 at the start of every batch \n",
    "# then we can basically calculate the mean loss and can print it \n",
    "# then we can do model.eval() \n",
    "# when we want to perform validation it is necessary as it gives us idea that our model is overfitting or not. \n",
    "# remember to use with torch.no_grad() when calculation and updating val metric as we dont want to change our original computation graph which we are builidng for our model. \n",
    "# finally we can print everything and do this again for next epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9933f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why create optimizer after moving model to gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba323d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllibraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
